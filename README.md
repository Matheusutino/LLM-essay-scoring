# From Zero-shot to Self-generated References: Leveraging LLMs for Scoring ENEM Essays

This repository contains the source code, prompts, and experimental configurations for the paper "From Zero-shot to Self-generated References: Leveraging LLMs for Scoring ENEM Essays", submitted to STIL 2025.

## ğŸ§  Overview

We explore the use of Large Language Models (LLMs) for automated essay scoring (AES) in the context of the Brazilian high school national exam (ENEM). Our study compares three prompting strategies:

- âœ… Zero-shot: model receives only the theme and student essay
- âœ… One-shot: model is also provided with the highest-scoring real essay for the same topic
- âœ… Self-generated reference (novel approach): model generates its own ideal reference essay (score 1000) and uses it for comparison

Each essay is scored across five official ENEM competencies, covering grammar, topic understanding, argumentation, cohesion, and proposal of intervention.

## ğŸ” Key Findings

- ğŸ¥‡ One-shot prompting achieves the highest overall performance across all models and competencies.
- âœï¸ The self-generated reference strategy performs comparably to one-shot in some scenarios, offering a viable alternative when no real references are available.
- âš–ï¸ LLMs outperform traditional AES methods (e.g., handcrafted features + regression) in both classification (QWK) and regression (RMSE) tasks.
- ğŸ’¬ Models also produce detailed justifications for each score, as well as general feedback to support pedagogical use.
- âš¡ Even smaller LLMs (like Qwen3-8B) show competitive results in regression settings, highlighting potential in low-resource scenarios.

## ğŸ“Š Dataset

We use the publicly available Essay-BR Corpus, which includes:

- 4,570 ENEM-style essays written by Brazilian students
- Manual annotations based on the official ENEM scoring rubric
- Subscores for each of the five competencies
- Topics include politics, public health, human rights, and other social issues

The corpus is split into:

- 70% training
- 15% validation
- 15% test

## ğŸ“ Repository Structure

- configs/ â€” Prompt templates and some variable definitions
- datasets/ â€” Essay-BR Corpus dataset
- src/analysis/ â€” Analyze the results generated by the LLMs
- src/core/llm_predictor/ â€” Call LLM APIs (e.g., NVIDIA, ChutesAI)
- src/scripts/ â€” Runners for essay scoring and generation workflows (zero-shot, one-shot, and self-generated)
- results/analysis â€” Contains the results obtained from files in src/analysis/
- results/essay â€” Evaluation results for the three settings (zero-shot, one-shot, and self-generated)
- results/generated â€” Essays ideally with maximum scores generated by the LLMs
- supplementary_materials/ â€” Supplementary material with the original prompts and their translated English versions

## ğŸ“ Supplementary Materials

- [Prompting templates for each setting (zero-shot, one-shot, self-generated)](./supplementary_materials/prompts.pdf)

## ğŸ”¬ Citation

Coming soon (after peer review).

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ“¬ Contact

For questions or collaboration inquiries, please contact:  
[REMOVED FOR DOUBLE BLIND REVIEW]
